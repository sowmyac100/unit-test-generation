{
  "model_name": "meta-llama/Llama-2-70b-hf",
  "use_lora": true,
  "lora_r": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.1,
  "target_modules": ["q_proj", "v_proj"],
  "bias": "none",
  "task_type": "CAUSAL_LM",
  "quantization": true,
  "output_file": "generated_unit_tests.py"
}
